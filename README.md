E-Commerce Microservices Platform - Kubernetes Deployment

## Repositorio Original
Este proyecto es un fork del repositorio original:  
[üìå Ver repositorio original]([https://github.com/autor/repo-original)](https://github.com/SelimHorri/ecommerce-microservice-backend-app)

https://helm.sh/
https://kubernetes.io/

üìã Tabla de Contenidos

Descripci√≥n General
Arquitectura
Caracter√≠sticas Principales
Requisitos Previos
Instalaci√≥n
Estructura del Proyecto
Configuraci√≥n de Servicios
Network Policies
Monitoreo y Observabilidad
Seguridad
CI/CD
Estrategias de Despliegue
Operaciones
Troubleshooting


üéØ Descripci√≥n General
Plataforma de e-commerce basada en microservicios desplegada en Kubernetes utilizando Helm charts. El proyecto implementa patrones modernos de arquitectura cloud-native incluyendo:

Microservicios Spring Boot con patr√≥n API Gateway
Service Discovery con Eureka
Configuraci√≥n centralizada con Spring Cloud Config
Observabilidad completa con Prometheus, Grafana y Jaeger
Seguridad robusta con Network Policies, Sealed Secrets y RBAC
CI/CD automatizado con GitHub Actions
M√∫ltiples estrategias de despliegue (Blue-Green, Canary)


üèóÔ∏è Arquitectura
```bash
Diagrama de Arquitectura
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ   Internet      ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                             ‚îÇ
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ Ingress (NGINX) ‚îÇ
                                    ‚îÇ   TLS/HTTPS     ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                             ‚îÇ
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ  API Gateway    ‚îÇ
                                    ‚îÇ   (Port 8080)   ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                             ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ                            ‚îÇ                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ User Service ‚îÇ          ‚îÇ Product Service  ‚îÇ        ‚îÇ Order Service    ‚îÇ
        ‚îÇ  (Port 8700) ‚îÇ          ‚îÇ   (Port 8500)    ‚îÇ        ‚îÇ  (Port 8300)     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ                           ‚îÇ                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   MySQL DB   ‚îÇ          ‚îÇ    MySQL DB      ‚îÇ        ‚îÇ    MySQL DB      ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Infrastructure Services                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Service Discovery  ‚îÇ  Cloud Config  ‚îÇ   Jaeger   ‚îÇ  Prometheus  ‚îÇ  Grafana ‚îÇ
‚îÇ     (Eureka)        ‚îÇ   (Port 9296)  ‚îÇ (Tracing)  ‚îÇ  (Metrics)   ‚îÇ   (UI)   ‚îÇ
‚îÇ    (Port 8761)      ‚îÇ                ‚îÇ            ‚îÇ              ‚îÇ           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```


‚ú® Caracter√≠sticas Principales
üîí Seguridad

Pod Security Standards: Pol√≠ticas baseline (dev/qa) y restricted (prod)
Network Policies: Segmentaci√≥n de red granular
Sealed Secrets: Encriptaci√≥n de secretos con Bitnami Sealed Secrets
RBAC: Control de acceso basado en roles
TLS/HTTPS: Certificados para endpoints p√∫blicos
Security Contexts: Ejecuci√≥n sin privilegios, filesystem read-only

üìä Observabilidad

M√©tricas: Spring Boot Actuator + Prometheus
Visualizaci√≥n: Dashboards personalizados en Grafana
Trazabilidad: Jaeger con compatibilidad Zipkin
Logging: Logs centralizados por servicio
Alertas: Sistema de alertas en Prometheus

üöÄ Deployment

Blue-Green: Para servicios cr√≠ticos (Cloud Config, Service Discovery)
Canary: Para servicios orientados al cliente
HPA: Autoescalado horizontal basado en CPU/memoria
Health Checks: Liveness y readiness probes
Rolling Updates: Actualizaciones sin downtime

üîÑ CI/CD

GitHub Actions: Pipeline completo automatizado
Escaneo de vulnerabilidades: Trivy para im√°genes Docker
Validaci√≥n de Helm: Lint y template rendering
Testing: Pruebas automatizadas en cluster Kind
Multi-ambiente: Despliegue en dev, qa, prod


üì¶ Requisitos Previos
Software Necesario
bash# Kubernetes
kubectl >= 1.24

# Helm
helm >= 3.10

# Docker (opcional, para desarrollo local)
docker >= 20.10

# Kubeseal (para gesti√≥n de secrets)
kubeseal >= 0.23.0
Cluster Kubernetes

M√≠nimo: 4 CPU, 8GB RAM
Recomendado: 8 CPU, 16GB RAM
Storage Class: standard disponible
Ingress Controller: NGINX instalado


üöÄ Instalaci√≥n
1. Instalaci√≥n R√°pida (Producci√≥n)
bash# Clonar repositorio
git clone <repository-url>
cd helm/ecommerce

# Instalar en producci√≥n
helm install my-ecommerce . -f values-prod.yaml

# Aplicar sealed secrets
kubectl apply -f ../secrets/sealedsecrets/prod/
2. Instalaci√≥n por Ambientes
Desarrollo (dev)
bashhelm install my-ecommerce . -f values-dev.yaml
Caracter√≠sticas:

Base de datos H2 en memoria
1 r√©plica por servicio
Pod Security: baseline
Sin persistencia de datos

QA/Staging
bashhelm install my-ecommerce . -f values-qa.yaml
kubectl apply -f ../secrets/sealedsecrets/qa/
Caracter√≠sticas:

MySQL persistente
1 r√©plica por servicio
Pod Security: baseline
PersistentVolumes de 10Gi

Producci√≥n
bashhelm install my-ecommerce . -f values-prod.yaml
kubectl apply -f ../secrets/sealedsecrets/prod/
Caracter√≠sticas:

MySQL persistente con backup
HPA habilitado
Pod Security: restricted
Network Policies estrictas

3. Instalaci√≥n de Monitoreo
bashcd helm/monitoring
helm install my-monitoring . -f values-monitoring.yaml
4. Verificaci√≥n
bash# Verificar pods
kubectl get pods -n <namespace>

# Verificar servicios
kubectl get svc -n <namespace>

# Verificar ingress
kubectl get ingress -n <namespace>

# Verificar HPA
kubectl get hpa -n <namespace>

```bash

## üìÅ Estructura del Proyecto

helm/
‚îú‚îÄ‚îÄ ecommerce/                          # Chart principal (umbrella)
‚îÇ   ‚îú‚îÄ‚îÄ Chart.yaml                      # Metadata del chart
‚îÇ   ‚îú‚îÄ‚îÄ values-dev.yaml                 # Valores para desarrollo
‚îÇ   ‚îú‚îÄ‚îÄ values-qa.yaml                  # Valores para QA
‚îÇ   ‚îú‚îÄ‚îÄ values-prod.yaml                # Valores para producci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ namespace.yaml              # Definici√≥n de namespace
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ networkpolicies.yaml        # Network Policies
‚îÇ   ‚îî‚îÄ‚îÄ charts/                         # Subcharts de microservicios
‚îÇ       ‚îú‚îÄ‚îÄ api-gateway/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Chart.yaml
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ values.yaml
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ files/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gateway.crt         # Certificado TLS
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gateway.key         # Llave privada TLS
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ configmap.yaml
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ deployment-stable.yaml
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ deployment-canary.yaml
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ service.yaml
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ ingress.yaml
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ hpa.yaml
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ service-account.yaml
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ tls-secret.yaml
‚îÇ       ‚îú‚îÄ‚îÄ user-service/
‚îÇ       ‚îú‚îÄ‚îÄ product-service/
‚îÇ       ‚îú‚îÄ‚îÄ order-service/
‚îÇ       ‚îú‚îÄ‚îÄ payment-service/
‚îÇ       ‚îú‚îÄ‚îÄ shipping-service/
‚îÇ       ‚îú‚îÄ‚îÄ favourite-service/
‚îÇ       ‚îú‚îÄ‚îÄ proxy-client/
‚îÇ       ‚îú‚îÄ‚îÄ service-discovery/
‚îÇ       ‚îú‚îÄ‚îÄ cloud-config/
‚îÇ       ‚îú‚îÄ‚îÄ jaeger/
‚îÇ       ‚îî‚îÄ‚îÄ locust/
‚îú‚îÄ‚îÄ monitoring/                         # Monitoreo y observabilidad
‚îÇ   ‚îú‚îÄ‚îÄ Chart.yaml
‚îÇ   ‚îú‚îÄ‚îÄ values-monitoring.yaml
‚îÇ   ‚îú‚îÄ‚îÄ charts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ configmap.yaml      # Scrape configs
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ clusterrole.yaml
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ clusterrolebinding.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ dashboards/             # Dashboards JSON
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ springboot-metrics.json
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ eureka.json
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ jaeger.json
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ provisioning/           # Datasources y dashboards
‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ       ‚îî‚îÄ‚îÄ networkpolicies.yaml
‚îî‚îÄ‚îÄ secrets/                            # Gesti√≥n de secretos
    ‚îú‚îÄ‚îÄ sealedSecrets.sh               # Script para generar sealed secrets
    ‚îú‚îÄ‚îÄ mycert.pem                     # Certificado p√∫blico del controlador
    ‚îî‚îÄ‚îÄ sealedsecrets/
        ‚îú‚îÄ‚îÄ prod/
        ‚îÇ   ‚îú‚îÄ‚îÄ user-service-db.yaml
        ‚îÇ   ‚îú‚îÄ‚îÄ product-service-db.yaml
        ‚îÇ   ‚îî‚îÄ‚îÄ ...
        ‚îî‚îÄ‚îÄ qa/
            ‚îî‚îÄ‚îÄ ...
```
‚öôÔ∏è Configuraci√≥n de Servicios
ConfigMaps
Los ConfigMaps almacenan configuraci√≥n no sensible inyectada como variables de entorno:

```yaml
yamlapiVersion: v1
kind: ConfigMap
metadata:
  name: my-ecommerce-user-service-config
data:
  SPRING_PROFILES_ACTIVE: "prod"
  SPRING_ZIPKIN_BASE_URL: "http://my-ecommerce-zipkin:9411"
  SPRING_CONFIG_IMPORT: "optional:configserver:http://my-ecommerce-cloud-config:9296/"
  EUREKA_CLIENT_SERVICE_URL_DEFAULTZONE: "http://my-ecommerce-service-discovery:8761/eureka/"
  EUREKA_INSTANCE_PREFER_IP_ADDRESS: "true"
```
Secrets (Sealed Secrets)
Las credenciales de base de datos se gestionan con Sealed Secrets:
bash# Generar sealed secrets
./helm/secrets/sealedSecrets.sh

# Aplicar en el cluster
kubectl apply -f helm/secrets/sealedsecrets/prod/
Estructura de un Sealed Secret:
```yaml
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: user-service-db-credentials
  namespace: prod
spec:
  encryptedData:
    DB_USER: AgBYPKY6A8K0gYgB0JOujJ0...
    DB_PASSWORD: AgCU2JZE+o/Ix41cEd68dNo...
Service Accounts
Cada microservicio tiene su propio ServiceAccount con permisos m√≠nimos:
yamlapiVersion: v1
kind: ServiceAccount
metadata:
  name: my-ecommerce-user-service
  namespace: prod
automountServiceAccountToken: false  # Seguridad adicional
Horizontal Pod Autoscaler (HPA)
Configuraci√≥n de autoescalado:
yamlapiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-ecommerce-user-service-hpa-stable
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-ecommerce-user-service-stable
  minReplicas: 1
  maxReplicas: 2
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
```

üîê Network Policies
Modelo de Seguridad
El proyecto implementa un modelo de Zero Trust con las siguientes capas:

Default Deny All: Todo el tr√°fico bloqueado por defecto
Allow DNS: Resoluci√≥n de nombres permitida
Pol√≠ticas espec√≠ficas por servicio: Solo tr√°fico necesario

Tabla de Network Policies

| **Servicio**          | **Ingress (Origen ‚Üí Puerto)**                                                | **Egress (Destino ‚Üí Puerto)**                                                                                                                    |
|-----------------------|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|
| **API Gateway**       | Any source ‚Üí 8080                                     | User Service ‚Üí 8080, Product Service ‚Üí 8080, Order Service ‚Üí 8080, Payment Service ‚Üí 8080, Shipping Service ‚Üí 8080, Favourite Service ‚Üí 8080, DNS ‚Üí 53, Cloud Config ‚Üí 9296, Jaeger ‚Üí 9411, Eureka ‚Üí 8761 |
| **User Service**      | API Gateway ‚Üí 8700, Favourite Service ‚Üí 8700                                 | User DB ‚Üí 3306, Eureka ‚Üí 8761, Cloud Config ‚Üí 9296, Jaeger ‚Üí 9411, DNS ‚Üí 53                                                                     |
| **Product Service**   | API Gateway ‚Üí 8500, Favourite Service ‚Üí 8500, Shipping Service ‚Üí 8500, Proxy Client ‚Üí 8500 | Product DB ‚Üí 3306, Eureka ‚Üí 8761, Cloud Config ‚Üí 9296, Jaeger ‚Üí 9411, DNS ‚Üí 53                                                 |
| **Order Service**     | API Gateway ‚Üí 8300, Payment Service ‚Üí 8300, Shipping Service ‚Üí 8300          | Order DB ‚Üí 3306, Eureka ‚Üí 8761, Cloud Config ‚Üí 9296, Jaeger ‚Üí 9411, DNS ‚Üí 53                                                                     |
| **Payment Service**   | API Gateway ‚Üí 8400                                                           | Payment DB ‚Üí 3306, Order Service ‚Üí 8300, Eureka ‚Üí 8761, Cloud Config ‚Üí 9296, Jaeger ‚Üí 9411, DNS ‚Üí 53                                            |
| **Shipping Service**  | API Gateway ‚Üí 8600                                                           | Shipping DB ‚Üí 3306, Order Service ‚Üí 8300, Product Service ‚Üí 8500, Eureka ‚Üí 8761, Cloud Config ‚Üí 9296, Jaeger ‚Üí 9411, DNS ‚Üí 53                    |
| **Favourite Service** | API Gateway ‚Üí 8800                                                           | Favourite DB ‚Üí 3306, Product Service ‚Üí 8500, User Service ‚Üí 8700, Eureka ‚Üí 8761, Cloud Config ‚Üí 9296, Jaeger ‚Üí 9411, DNS ‚Üí 53                   |
| **Proxy Client**      | API Gateway ‚Üí 8900                                                           | HTTP external ‚Üí 8900, DNS ‚Üí 53  |




Ejemplo de Network Policy

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: user-service-network-policy
  namespace: prod
spec:
  podSelector:
    matchLabels:
      app: user-service
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Desde API Gateway
    - from:
        - podSelector:
            matchLabels:
              app: api-gateway
      ports:
        - protocol: TCP
          port: 8700
    # Desde Prometheus (monitoreo)
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: monitoring
          podSelector:
            matchLabels:
              app: prometheus
  egress:
    # DNS
    - to:
        - namespaceSelector: {}
      ports:
        - protocol: UDP
          port: 53
    # Base de datos
    - to:
        - podSelector:
            matchLabels:
              app: user-service-db
      ports:
        - protocol: TCP
          port: 3306
    # Eureka
    - to:
        - podSelector:
            matchLabels:
              app: service-discovery
      ports:
        - protocol: TCP
          port: 8761
```
üìä Monitoreo y Observabilidad
Prometheus
Configuraci√≥n de Scraping:

```yaml
scrape_configs:
  # Spring Boot Actuator
  - job_name: "spring-boot-actuator"
    kubernetes_sd_configs:
      - role: pod
        namespaces:
          names: [dev, qa, prod]
    relabel_configs:
      # Solo pods con prometheus.io/scrape=true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      # Construir endpoint
      - source_labels: [__meta_kubernetes_pod_ip, __meta_kubernetes_pod_annotation_prometheus_io_port]
        separator: ":"
        regex: (.+);(.+)
        replacement: "$1:$2"
        target_label: __address__

  # Jaeger
  - job_name: "jaeger"
    static_configs:
      - targets: ['jaeger:14269']

  # Eureka
  - job_name: "eureka"
    static_configs:
      - targets: ['service-discovery:8761']
```
Grafana Dashboards
El proyecto incluye 3 dashboards predefinidos:
1. Spring Boot Metrics Dashboard
Paneles principales:

HTTP Request Rate (req/s)
Average JVM Heap Usage (%)
Service Health (UP/DOWN)
Response Time Percentiles (p50, p95, p99)
HTTP Error Rates (4xx/5xx)
JVM Memory (Heap Used vs Max)
JVM Threads (Live/Daemon)
Pod CPU/Memory Usage
Liveness/Readiness Status
Garbage Collector Activity

Variables de Template:

$namespace: Filtrar por namespace
$service: Filtrar por servicio
$pod: Filtrar por pod

2. Eureka Service Discovery Dashboard
Paneles:

Eureka Server Status
Registered Instances
Heartbeats Received
Failed Registrations
Response Time
JVM Memory

3. Jaeger Tracing Dashboard
Paneles:

Received Spans
Dropped Spans
Queue Length
Spans by Transport
Ingest Errors
Jaeger Instances

Acceso a Interfaces
bash# Grafana (LoadBalancer)
kubectl get svc -n monitoring my-monitoring-grafana
# Credenciales por defecto: admin / admin123

### Prometheus
kubectl port-forward -n monitoring svc/my-monitoring-prometheus 9090:9090

### Jaeger UI
kubectl port-forward -n prod svc/my-ecommerce-zipkin 16686:16686

### Eureka Dashboard
kubectl get svc -n prod my-ecommerce-service-discovery

üõ°Ô∏è Seguridad
Pod Security Standards
| **Ambiente** | **Policy**   | **Caracter√≠sticas** |
|--------------|--------------|----------------------|
| **dev**      | baseline     | Permite contenedores privilegiados limitados, Filesystem parcialmente restringido, Ideal para desarrollo |
| **qa**       | baseline     | Configuraci√≥n similar a dev, Mayor auditor√≠a |
| **prod**     | restricted   | M√°xima seguridad, runAsNonRoot obligatorio, readOnlyRootFilesystem, Todas las capabilities eliminadas, Seccomp RuntimeDefault |


```yaml
yaml# Pod Level
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000
  seccompProfile:
    type: RuntimeDefault

# Container Level
containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
      - ALL
  seccompProfile:
    type: RuntimeDefault
```

Sealed Secrets Workflow
mermaidgraph LR
    A[Secret Plaintext] -->|kubeseal| B[Sealed Secret]
    B -->|kubectl apply| C[Kubernetes Cluster]
    C -->|Controller decrypt| D[Secret]
    D -->|Mount| E[Pod]
Comandos:
bash# 1. Obtener certificado p√∫blico
kubeseal --fetch-cert > mycert.pem

# 2. Crear secret y sellarlo
echo -n "mypassword" | kubectl create secret generic my-secret \
  --dry-run=client \
  --from-file=password=/dev/stdin \
  -o yaml | kubeseal --cert mycert.pem -o yaml > sealed-secret.yaml

# 3. Aplicar
kubectl apply -f sealed-secret.yaml
Escaneo de Vulnerabilidades
El pipeline CI/CD incluye escaneo con Trivy:

```yaml
- name: Scan Docker image with Trivy
  uses: aquasecurity/trivy-action@0.11.2
  with:
    image-ref: ${{ env.DOCKER_USERNAME }}/${{ matrix.service }}:${{ steps.scan-tag.outputs.SCAN_TAG }}
    format: 'template'
    template: '@/contrib/html.tpl'
    output: 'trivy-${{ matrix.service }}.html'
    vuln-type: 'os,library'
    severity: 'HIGH,CRITICAL'
```

üîÑ CI/CD
Pipeline GitHub Actions
El workflow se compone de 4 jobs principales:
mermaidgraph TD
    A[Push/PR] --> B[Build Maven]
    B --> C[Build & Scan Images]
    C --> D[Validate Helm Charts]
    D --> E[Deploy & Test]
    
    B --> B1[Maven Compile]
    B --> B2[Run Tests]
    B --> B3[Upload Artifacts]
    
    C --> C1[Build Docker Image]
    C --> C2[Push to Registry]
    C --> C3[Trivy Scan]
    
    D --> D1[Helm Lint]
    D --> D2[Template Render]
    
    E --> E1[Kind Cluster]
    E --> E2[Helm Install]
    E --> E3[Verify Deployment]
Job 1: Build Maven
Estrategia: Matrix paralela (10 servicios)
yamlstrategy:
  matrix:
    service:
      - user-service
      - product-service
      - payment-service
      # ... otros servicios
Pasos:

‚úÖ Detectar cambios (git diff)
‚úÖ Compilar con Maven
‚úÖ Ejecutar tests
‚úÖ Subir artifacts (.jar)

Job 2: Build & Scan Images
Condicional: Solo en push (no en PRs)
yamlif: github.event_name == 'push'
Pasos:

‚úÖ Descargar artifacts de Maven
‚úÖ Build imagen Docker
‚úÖ Push a Docker Hub (tags: feature-X y latest)
‚úÖ Scan con Trivy (console + HTML report)

Output: Reporte HTML descargable como artifact
Job 3: Validate Helm Charts
yaml- name: Helm lint
  run: helm lint helm/ecommerce/charts/${{ matrix.service }} -f values-ci.yaml

- name: Helm template
  run: helm template ${{ matrix.service }} helm/ecommerce/charts/${{ matrix.service }} \
    -f values-ci.yaml > rendered.yaml
Job 4: Deploy & Test
Ambiente: Cluster Kind local
Flujo:

‚úÖ Crear cluster Kind
‚úÖ Detectar cambios por servicio
‚úÖ Desplegar con tags inteligentes:

Si cambi√≥ ‚Üí tag nuevo (feature-X)
Si no ‚Üí tag estable (feature-monitoring)


‚úÖ Espera ordenada (Eureka ‚Üí Config ‚Üí Servicios)
‚úÖ Verificar deployments
‚úÖ Debug logs si falla

Ejemplo de tag inteligente:
yaml# Si user-service cambi√≥
--set user-service.image.tag=${{ needs.set-env.outputs.IMAGE_TAG }}

# Si no cambi√≥
--set user-service.image.tag=feature-monitoring

üöÄ Estrategias de Despliegue
Blue-Green Deployment
Servicios: cloud-config, service-discovery, shipping-service, user-service
Ventajas:

‚úÖ Cambio instant√°neo
‚úÖ Rollback inmediato
‚úÖ Zero downtime

```yaml
Configuraci√≥n:
yaml# values.yaml
deploymentStrategy:
  type: blue-green
  active: blue  # o green

blue:
  image:
    repository: barcino/cloud-config
    tag: v1.0.0

green:
  image:
    repository: barcino/cloud-config
    tag: v1.1.0
Cambio de versi√≥n:
bash# Cambiar a green
helm upgrade my-ecommerce ./helm/ecommerce \
  -f values-prod.yaml \
  --set cloud-config.deploymentStrategy.active=green
```

Comportamiento:

Solo el deployment activo tiene r√©plicas > 0
El Service apunta al deployment activo v√≠a label color

Canary Deployment
Servicios: api-gateway, user-service, product-service, order-service, payment-service, favourite-service, proxy-client
Ventajas:

‚úÖ Prueba gradual con tr√°fico real
‚úÖ Menor riesgo
‚úÖ Feedback r√°pido

Configuraci√≥n:

```yaml
stable:
  image:
    repository: barcino/api-gateway
    tag: v1.0.0
  replicaCount: 3

canary:
  image:
  repository: barcino/api-gateway
  tag: v1.1.0
  replicaCount: 1  # 25% del tr√°fico
```
**Progresi√≥n**:
```bash
# 1. Introducir canary (10%)
helm upgrade my-ecommerce ./helm/ecommerce \
  --set api-gateway.canary.replicaCount=1 \
  --set api-gateway.stable.replicaCount=9

# 2. Aumentar canary (50%)
helm upgrade my-ecommerce ./helm/ecommerce \
  --set api-gateway.canary.replicaCount=5 \
  --set api-gateway.stable.replicaCount=5

# 3. Promover a stable
helm upgrade my-ecommerce ./helm/ecommerce \
  --set api-gateway.stable.image.tag=v1.1.0 \
  --set api-gateway.stable.replicaCount=10 \
  --set api-gateway.canary.replicaCount=0
```

---

## üõ†Ô∏è Operaciones

### Backup y Restore de Bases de Datos

**Script autom√°tico**: `helm/ecommerce/backup.sh`
```bash
# Backup de todas las bases de datos
./backup.sh
# Seleccionar opci√≥n: 1

# Restore
./backup.sh
# Seleccionar opci√≥n: 2
```

**Ubicaci√≥n**: `./backup/<servicio>/`

**Manual**:
```bash
# Backup de user-service
kubectl exec -n prod -it <user-service-db-pod> -- \
  mysqldump -u user -p12345 user-service_db | gzip > backup.sql.gz

# Restore
gunzip < backup.sql.gz | \
  kubectl exec -n prod -i <user-service-db-pod> -- \
    mysql -u user -p12345 user-service_db
```

### Verificaci√≥n de Persistencia
```bash
# 1. Insertar dato
kubectl exec -n prod -it <db-pod> -- mysql -u user -p12345 -e \
  "INSERT INTO user-service_db.users (name) VALUES ('test');"

# 2. Eliminar pod
kubectl delete pod -n prod <db-pod>

# 3. Verificar dato en nuevo pod
kubectl exec -n prod -it <nuevo-db-pod> -- mysql -u user -p12345 -e \
  "SELECT * FROM user-service_db.users WHERE name='test';"
```

### Logs
```bash
# Ver logs de un servicio
kubectl logs -n prod -l app=user-service --tail=100 -f

# Logs de init containers
kubectl logs -n prod <pod> -c wait-for-dependencies

# Logs de m√∫ltiples pods
kubectl logs -n prod -l app=user-service --all-containers=true
```

### Escalado Manual
```bash
# Escalar stable deployment
kubectl scale deployment -n prod my-ecommerce-user-service-stable --replicas=5

# V√≠a Helm
helm upgrade my-ecommerce ./helm/ecommerce \
  -f values-prod.yaml \
  --set user-service.stable.replicaCount=5
```

### Actualizar Cloud Config
```bash
# 1. Modificar configuraci√≥n en GitHub
git clone <cloud-config-repo>
cd cloud-config-repo
# Editar archivos...
git commit -am "Update config"
git push

# 2. Reiniciar pod para recargar
kubectl rollout restart deployment -n prod my-ecommerce-cloud-config-blue

# 3. Verificar
kubectl exec -n prod <pod> -- wget -qO- \
  http://my-ecommerce-cloud-config:9296/user-service/prod
```

---

## üîß Troubleshooting

### Problema: Pods en CrashLoopBackOff

**Diagn√≥stico**:
```bash
# Ver estado
kubectl get pods -n prod

# Ver eventos
kubectl describe pod -n prod <pod-name>

# Ver logs
kubectl logs -n prod <pod-name> --previous
```

**Causas comunes**:

1. **Dependencias no disponibles**:
```bash
   # Verificar Eureka
   kubectl get pods -n prod -l app=service-discovery
   
   # Verificar Cloud Config
   kubectl get pods -n prod -l app=cloud-config
```

2. **Secrets no aplicados**:
```bash
   kubectl get secrets -n prod | grep db-credentials
```
   Soluci√≥n:
```bash
   kubectl apply -f helm/secrets/sealedsecrets/prod/
```

3. **Network Policy bloqueando tr√°fico**:
```bash
   # Temporalmente deshabilitar para prueba
   kubectl delete networkpolicy -n prod <policy-name>
```

### Problema: HPA no escala

**Diagn√≥stico**:
```bash
kubectl get hpa -n prod
kubectl describe hpa -n prod my-ecommerce-user-service-hpa-stable
```

**Causas**:

1. **Metrics Server no instalado**:
```bash
   kubectl get deployment -n kube-system metrics-server
```
   Soluci√≥n:
```bash
   kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

2. **Recursos no definidos**:
   Verificar en `values.yaml`:
```yaml
   resources:
     requests:
       cpu: "500m"
       memory: "1700Mi"
```

### Problema: Sealed Secret no desencripta

**Diagn√≥stico**:
```bash
kubectl get sealedsecrets -n prod
kubectl describe sealedsecret -n prod <name>
kubectl logs -n kube-system -l name=sealed-secrets-controller
```

**Soluci√≥n**:
```bash
# Regenerar sealed secret con certificado correcto
kubeseal --fetch-cert > mycert.pem
echo -n "mypass" | kubectl create secret generic test \
  --dry-run=client --from-file=pass=/dev/stdin -o yaml | \
  kubeseal --cert mycert.pem -n prod -o yaml > sealed.yaml
kubectl apply -f sealed.yaml
```

### Problema: Prometheus no recolecta m√©tricas

**Diagn√≥stico**:
```bash
# Verificar targets en Prometheus UI
kubectl port-forward -n monitoring svc/my-monitoring-prometheus 9090:9090
# Abrir http://localhost:9090/targets
```

**Causas**:

1. **Anotaciones faltantes**:
   Verificar en deployment:
```yaml
   annotations:
     prometheus.io/scrape: "true"
     prometheus.io/port: "8700"
     prometheus.io/path: "/actuator/prometheus"
```

2. **Network Policy bloqueando Prometheus**:
```yaml
   - from:
       - namespaceSelector:
           matchLabels:
             kubernetes.io/metadata.name: monitoring
         podSelector:
           matchLabels:
             app: prometheus
```

### Problema: Ingress no resuelve

**Diagn√≥stico**:
```bash
kubectl get ingress -n prod
kubectl describe ingress -n prod my-ecommerce-api-gateway
```

**Soluci√≥n**:
```bash
# 1. Verificar Ingress Controller
kubectl get pods -n ingress-nginx

# 2. Agregar entrada a /etc/hosts (local)
echo "192.168.49.2 gateway.local" | sudo tee -a /etc/hosts

# 3. Verificar certificado TLS
kubectl get secret -n prod my-ecommerce-api-gateway-tls
```

---

## üìù Comandos √ötiles

### Helm
```bash
# Listar releases
helm list -A

# Ver valores computados
helm get values my-ecommerce -n prod

# Ver manifiesto completo
helm get manifest my-ecommerce -n prod

# Dry-run para validar
helm install my-ecommerce ./helm/ecommerce -f values-prod.yaml --dry-run --debug

# Rollback
helm rollback my-ecommerce 1 -n prod

# Actualizar una sola variable
helm upgrade my-ecommerce ./helm/ecommerce -f values-prod.yaml \
  --set user-service.stable.replicaCount=5 \
  --reuse-values
```

### Kubectl
```bash
# Port-forward m√∫ltiple
kubectl port-forward -n prod svc/my-ecommerce-service-discovery 8761:8761 &
kubectl port-forward -n prod svc/my-ecommerce-api-gateway 8080:8080 &

# Ejecutar comando en pod
kubectl exec -n prod -it <pod> -- /bin/sh

# Copiar archivos
kubectl cp -n prod <pod>:/path/to/file ./local-file

# Ver recursos consumidos
kubectl top pods -n prod
kubectl top nodes

# Eventos del namespace
kubectl get events -n prod --sort-by='.lastTimestamp'

# Restart de deployments
kubectl rollout restart deployment -n prod my-ecommerce-user-service-stable

# Ver hist√≥rico de rollouts
kubectl rollout history deployment -n prod my-ecommerce-user-service-stable
```

---

## üë• Autores

## Repositorio Original
Este proyecto es un fork del repositorio original:  
[üìå Ver repositorio original]([https://github.com/autor/repo-original)](https://github.com/SelimHorri/ecommerce-microservice-backend-app)

Este es una adaptaci√≥n al proyecto original de

- **Juan Jos√© Barrera Gracia**
- **Andr√©s Mauricio Mesa Franco**

**Universidad ICESI**  
Facultad Barberi de Ingenier√≠a y Dise√±o  
Ingenier√≠a Telem√°tica  
2025

